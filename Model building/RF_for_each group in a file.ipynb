{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics,ensemble\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_log_error as MSLE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Desktop\\\\FINTECH'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time, glob, socket\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "random.seed(123)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'D:\\fintech')\n",
    "path=os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  按照 purpose分 dataframe 之後，依序讀進來\n",
    "- 再將 train test 的 X_num_train  X_char_train  Y_train 分開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groupname=os.listdir(path+'\\group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'credit_card',\n",
       " 'debt_consolidation',\n",
       " 'home_improvement',\n",
       " 'house',\n",
       " 'major_purchase',\n",
       " 'medical',\n",
       " 'moving',\n",
       " 'others',\n",
       " 'small_business',\n",
       " 'vacation']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 0.7, 'n_estimators': 61}\n"
     ]
    }
   ],
   "source": [
    "# 這裡先用 car 的\n",
    "i=5\n",
    "\n",
    "X_num_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_train_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_num_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_test_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "    \n",
    "X_char_train_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_train_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "X_char_test_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_test_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_train=pd.merge(X_num_train,X_char_train_dummy)\n",
    "X_test=pd.merge(X_num_test,X_char_test_dummy)\n",
    "\n",
    "\n",
    "y_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_train_\"+str(groupname[i])+\".csv\")\n",
    "y_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_test_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "## random forest\n",
    "\n",
    "#drop id/earch\n",
    "\n",
    "gsc = GridSearchCV(estimator=RandomForestClassifier(),param_grid={\n",
    "            'max_features':['sqrt','auto','log2',0.5,0.7],\n",
    "            'max_depth': range(1,6),\n",
    "            'n_estimators': range(1,101,10)},\n",
    "             cv=10, scoring='f1',verbose=0,n_jobs=-1,)\n",
    "\n",
    "grid_result = gsc.fit(X_train,y_train.ans)\n",
    "best_params = grid_result.best_params_\n",
    "print(best_params)\n",
    "\n",
    "rfr = RandomForestClassifier(\n",
    "    max_depth=best_params[\"max_depth\"], \n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=False, verbose=False)\n",
    "\n",
    "scores = cross_val_score(rfr, X_train,y_train.ans, cv=10, scoring='f1')\n",
    "rfr.fit(X_train,y_train.ans)\n",
    "\n",
    "\n",
    "# save model\n",
    "#儲存Model(注:save資料夾要預先建立，否則會報錯)\n",
    "with open( path+'\\group'+'\\\\'+str(groupname[i])+'\\\\'+'rfr_'+str(groupname[i])+'.pickle', 'wb') as f:\n",
    "    pickle.dump(rfr, f)\n",
    "\n",
    "#讀取Model\n",
    "#with open('save/clf.pickle', 'rb') as f:\n",
    "#    clf2 = pickle.load(f)\n",
    "    #測試讀取後的Model\n",
    "#   print(clf2.predict(X[0:1]))\n",
    "\n",
    "## predict_y\n",
    "\n",
    "rf_1_pred=rfr.predict(X_test)\n",
    "\n",
    "y_ans = pd.DataFrame(rf_1_pred,columns=['predict'],index=y_test.index)\n",
    "y_output=pd.merge(y_test,y_ans,left_index=True,right_index=True)\n",
    "y_output.to_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_output_rf_\"+str(groupname[i])+ \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 0.7, 'n_estimators': 61}\n"
     ]
    }
   ],
   "source": [
    "# 這裡先用 car 的\n",
    "i=6\n",
    "\n",
    "X_num_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_train_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_num_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_test_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "    \n",
    "X_char_train_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_train_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "X_char_test_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_test_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_train=pd.merge(X_num_train,X_char_train_dummy)\n",
    "X_test=pd.merge(X_num_test,X_char_test_dummy)\n",
    "\n",
    "\n",
    "y_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_train_\"+str(groupname[i])+\".csv\")\n",
    "y_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_test_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "## random forest\n",
    "\n",
    "#drop id\n",
    "X_train=X_train.drop(['Unnamed: 0.1'], axis=1)\n",
    "X_test=X_test.drop(['Unnamed: 0.1'], axis=1)\n",
    "\n",
    "\n",
    "# Perform Grid-Search\n",
    "\n",
    "gsc = GridSearchCV(estimator=RandomForestClassifier(),param_grid={\n",
    "            'max_features':['sqrt','auto','log2',0.5,0.7],\n",
    "            'max_depth': range(1,6),\n",
    "            'n_estimators': range(1,101,10)},\n",
    "             cv=10, scoring='f1',verbose=0,n_jobs=-1,)\n",
    "\n",
    "grid_result = gsc.fit(X_train,y_train.ans)\n",
    "best_params = grid_result.best_params_\n",
    "print(best_params)\n",
    "\n",
    "rfr = RandomForestClassifier(\n",
    "    max_depth=best_params[\"max_depth\"], \n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=False, verbose=False)\n",
    "\n",
    "scores = cross_val_score(rfr, X_train,y_train.ans, cv=10, scoring='f1')\n",
    "rfr.fit(X_train,y_train.ans)\n",
    "\n",
    "\n",
    "# save model\n",
    "#儲存Model(注:save資料夾要預先建立，否則會報錯)\n",
    "with open( path+'\\group'+'\\\\'+str(groupname[i])+'\\\\'+'rfr_'+str(groupname[i])+'.pickle', 'wb') as f:\n",
    "    pickle.dump(rfr, f)\n",
    "\n",
    "#讀取Model\n",
    "#with open('save/clf.pickle', 'rb') as f:\n",
    "#    clf2 = pickle.load(f)\n",
    "    #測試讀取後的Model\n",
    "#   print(clf2.predict(X[0:1]))\n",
    "\n",
    "## predict_y\n",
    "\n",
    "rf_1_pred=rfr.predict(X_test)\n",
    "\n",
    "y_ans = pd.DataFrame(rf_1_pred,columns=['predict'],index=y_test.index)\n",
    "y_output=pd.merge(y_test,y_ans,left_index=True,right_index=True)\n",
    "y_output.to_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_output_rf_\"+str(groupname[i])+ \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 0.7, 'n_estimators': 31}\n"
     ]
    }
   ],
   "source": [
    "# 這裡先用 car 的\n",
    "i=3\n",
    "\n",
    "X_num_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_train_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_num_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_test_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "    \n",
    "X_char_train_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_train_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "X_char_test_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_test_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_train=pd.merge(X_num_train,X_char_train_dummy)\n",
    "X_test=pd.merge(X_num_test,X_char_test_dummy)\n",
    "\n",
    "\n",
    "y_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_train_\"+str(groupname[i])+\".csv\")\n",
    "y_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_test_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "## random forest\n",
    "\n",
    "#drop id\n",
    "X_train=X_train.drop(['Unnamed: 0.1'], axis=1)\n",
    "X_test=X_test.drop(['Unnamed: 0.1'], axis=1)\n",
    "\n",
    "\n",
    "# Perform Grid-Search\n",
    "\n",
    "gsc = GridSearchCV(estimator=RandomForestClassifier(),param_grid={\n",
    "            'max_features':['sqrt','auto','log2',0.5,0.7],\n",
    "            'max_depth': range(1,6),\n",
    "            'n_estimators': range(1,101,10)},\n",
    "             cv=10, scoring='f1',verbose=0,n_jobs=-1,)\n",
    "\n",
    "grid_result = gsc.fit(X_train,y_train.ans)\n",
    "best_params = grid_result.best_params_\n",
    "print(best_params)\n",
    "\n",
    "rfr = RandomForestClassifier(\n",
    "    max_depth=best_params[\"max_depth\"], \n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=False, verbose=False)\n",
    "\n",
    "scores = cross_val_score(rfr, X_train,y_train.ans, cv=10, scoring='f1')\n",
    "rfr.fit(X_train,y_train.ans)\n",
    "\n",
    "\n",
    "# save model\n",
    "#儲存Model(注:save資料夾要預先建立，否則會報錯)\n",
    "with open( path+'\\group'+'\\\\'+str(groupname[i])+'\\\\'+'rfr_'+str(groupname[i])+'.pickle', 'wb') as f:\n",
    "    pickle.dump(rfr, f)\n",
    "\n",
    "#讀取Model\n",
    "#with open('save/clf.pickle', 'rb') as f:\n",
    "#    clf2 = pickle.load(f)\n",
    "    #測試讀取後的Model\n",
    "#   print(clf2.predict(X[0:1]))\n",
    "\n",
    "## predict_y\n",
    "\n",
    "rf_1_pred=rfr.predict(X_test)\n",
    "\n",
    "y_ans = pd.DataFrame(rf_1_pred,columns=['predict'],index=y_test.index)\n",
    "y_output=pd.merge(y_test,y_ans,left_index=True,right_index=True)\n",
    "y_output.to_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_output_rf_\"+str(groupname[i])+ \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'max_features': 0.7, 'n_estimators': 11}\n"
     ]
    }
   ],
   "source": [
    "# 這裡先用 car 的\n",
    "i=4\n",
    "\n",
    "X_num_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_train_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_num_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_test_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "    \n",
    "X_char_train_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_train_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "X_char_test_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_test_dummy_\"+str(groupname[i])+\".csv\",index_col=0)\n",
    "\n",
    "X_train=pd.merge(X_num_train,X_char_train_dummy)\n",
    "X_test=pd.merge(X_num_test,X_char_test_dummy)\n",
    "\n",
    "\n",
    "y_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_train_\"+str(groupname[i])+\".csv\")\n",
    "y_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_test_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "## random forest\n",
    "\n",
    "#drop id\n",
    "X_train=X_train.drop(['Unnamed: 0.1'], axis=1)\n",
    "X_test=X_test.drop(['Unnamed: 0.1'], axis=1)\n",
    "\n",
    "\n",
    "# Perform Grid-Search\n",
    "\n",
    "gsc = GridSearchCV(estimator=RandomForestClassifier(),param_grid={\n",
    "            'max_features':['sqrt','auto','log2',0.5,0.7],\n",
    "            'max_depth': range(1,6),\n",
    "            'n_estimators': range(1,101,10)},\n",
    "             cv=10, scoring='f1',verbose=0,n_jobs=-1,)\n",
    "\n",
    "grid_result = gsc.fit(X_train,y_train.ans)\n",
    "best_params = grid_result.best_params_\n",
    "print(best_params)\n",
    "\n",
    "rfr = RandomForestClassifier(\n",
    "    max_depth=best_params[\"max_depth\"], \n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_features=best_params['max_features'],\n",
    "    random_state=False, verbose=False)\n",
    "\n",
    "scores = cross_val_score(rfr, X_train,y_train.ans, cv=10, scoring='f1')\n",
    "rfr.fit(X_train,y_train.ans)\n",
    "\n",
    "\n",
    "# save model\n",
    "#儲存Model(注:save資料夾要預先建立，否則會報錯)\n",
    "with open( path+'\\group'+'\\\\'+str(groupname[i])+'\\\\'+'rfr_'+str(groupname[i])+'.pickle', 'wb') as f:\n",
    "    pickle.dump(rfr, f)\n",
    "\n",
    "#讀取Model\n",
    "#with open('save/clf.pickle', 'rb') as f:\n",
    "#    clf2 = pickle.load(f)\n",
    "    #測試讀取後的Model\n",
    "#   print(clf2.predict(X[0:1]))\n",
    "\n",
    "## predict_y\n",
    "\n",
    "rf_1_pred=rfr.predict(X_test)\n",
    "\n",
    "y_ans = pd.DataFrame(rf_1_pred,columns=['predict'],index=y_test.index)\n",
    "y_output=pd.merge(y_test,y_ans,left_index=True,right_index=True)\n",
    "y_output.to_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_output_rf_\"+str(groupname[i])+ \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(groupname)):\n",
    "    X_num_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_train_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "    X_num_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_num_test_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "    X_char_train_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_train_dummy_\"+str(groupname[i])+\".csv\")\n",
    "    X_char_test_dummy=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\X_char_test_dummy_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "    X_train=pd.merge(X_num_train,X_char_train_dummy)\n",
    "    X_test=pd.merge(X_num_test,X_char_test_dummy)\n",
    "\n",
    "\n",
    "    y_train=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_train_\"+str(groupname[i])+\".csv\")\n",
    "    y_test=pd.read_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_test_\"+str(groupname[i])+\".csv\")\n",
    "\n",
    "\n",
    "\n",
    "    ## random forest\n",
    "\n",
    "    #drop id\n",
    "    X_train=X_train.drop(['Unnamed: 0.1'], axis=1)\n",
    "    X_test=X_test.drop(['Unnamed: 0.1'], axis=1)\n",
    "\n",
    "\n",
    "    # Perform Grid-Search\n",
    "\n",
    "    gsc = GridSearchCV(estimator=RandomForestClassifier(),param_grid={\n",
    "                'max_features':['sqrt','auto','log2',0.5,0.7],\n",
    "                'max_depth': range(1,6),\n",
    "                'n_estimators': range(1,101,10)},\n",
    "                 cv=10, scoring='f1',verbose=0,n_jobs=-1,)\n",
    "\n",
    "    grid_result = gsc.fit(X_train,y_train.ans)\n",
    "    best_params = grid_result.best_params_\n",
    "    print(best_params)\n",
    "\n",
    "    rfr = RandomForestClassifier(\n",
    "        max_depth=best_params[\"max_depth\"], \n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        max_features=best_params['max_features'],\n",
    "        random_state=False, verbose=False)\n",
    "\n",
    "    scores = cross_val_score(rfr, X_train,y_train.ans, cv=10, scoring='f1')\n",
    "    rfr.fit(X_train,y_train.ans)\n",
    "    \n",
    "    # save model\n",
    "    #儲存Model(注:save資料夾要預先建立，否則會報錯)\n",
    "    with open( path+'\\group'+'\\\\'+str(groupname[i])+'\\\\'+'rfr_'+str(groupname[i])+'.pickle', 'wb') as f:\n",
    "        pickle.dump(rfr, f)\n",
    "    \n",
    "    ## predict_y\n",
    "\n",
    "    rf_1_pred=rfr.predict(X_test)\n",
    "\n",
    "    y_ans = pd.DataFrame(rf_1_pred,columns=['predict'],index=y_test.index)\n",
    "    y_output=pd.merge(y_test,y_ans,left_index=True,right_index=True)\n",
    "    y_output.to_csv(path+'\\group'+'\\\\'+str(groupname[i])+\"\\\\y_output_rf_\"+str(groupname[i])+ \".csv\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\fintech'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'credit_card',\n",
       " 'debt_consolidation',\n",
       " 'home_improvement',\n",
       " 'house',\n",
       " 'major_purchase',\n",
       " 'medical',\n",
       " 'moving',\n",
       " 'others',\n",
       " 'small_business',\n",
       " 'vacation']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output_rfr_car=pd.read_csv(r'D:\\fintech\\group\\car\\y_output_rf_car.csv',index_col=0)\n",
    "y_output_rfr_credit_card=pd.read_csv(r'D:\\fintech\\group\\credit_card\\y_output_rf_credit_card.csv',index_col=0)\n",
    "y_output_rfr_debt_consolidation=pd.read_csv(r'D:\\fintech\\group\\debt_consolidation\\y_output_rf_debt_consolidation.csv',index_col=0)\n",
    "y_output_rfr_home_improvement=pd.read_csv(r'D:\\fintech\\group\\home_improvement\\y_output_rf_home_improvement.csv',index_col=0)\n",
    "y_output_rfr_house=pd.read_csv(r'D:\\fintech\\group\\house\\y_output_rf_house.csv',index_col=0)\n",
    "y_output_rfr_major_purchase=pd.read_csv(r'D:\\fintech\\group\\major_purchase\\y_output_rf_major_purchase.csv',index_col=0)\n",
    "y_output_rfr_medical=pd.read_csv(r'D:\\fintech\\group\\medical\\y_output_rf_medical.csv',index_col=0)\n",
    "y_output_rfr_moving=pd.read_csv(r'D:\\fintech\\group\\moving\\y_output_rf_moving.csv',index_col=0)\n",
    "y_output_rfr_others=pd.read_csv(r'D:\\fintech\\group\\others\\y_output_rf_others.csv',index_col=0)\n",
    "y_output_rfr_small_business=pd.read_csv(r'D:\\fintech\\group\\small_business\\y_output_rf_small_business.csv',index_col=0)\n",
    "y_output_rfr_vacation=pd.read_csv(r'D:\\fintech\\group\\vacation\\y_output_rf_vacation.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output_rfr_car=y_output_rfr_car.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_credit_card=y_output_rfr_credit_card.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_debt_consolidation=y_output_rfr_debt_consolidation.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_home_improvement=y_output_rfr_home_improvement.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_house=y_output_rfr_house.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_major_purchase=y_output_rfr_major_purchase.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_medical=y_output_rfr_medical.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_moving=y_output_rfr_moving.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_others=y_output_rfr_others.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_small_business=y_output_rfr_small_business.drop(['Unnamed: 0.1'],axis=1)\n",
    "y_output_rfr_vacation=y_output_rfr_vacation.drop(['Unnamed: 0.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output=pd.concat([y_output_rfr_car,y_output_rfr_credit_card,y_output_rfr_debt_consolidation,\n",
    "                   y_output_rfr_home_improvement,y_output_rfr_house,y_output_rfr_major_purchase,\n",
    "                   y_output_rfr_medical,y_output_rfr_moving,y_output_rfr_others,y_output_rfr_small_business,\n",
    "                   y_output_rfr_vacation],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output=y_output.sort_values(by='Unnamed: 0.1.1', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output=y_output.drop(['Unnamed: 0.1.1'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test=pd.read_csv(\"Y_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dict = {1:'Y' ,0:'N'}\n",
    "y_output['predict'] = y_output['predict'].map(Dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_output.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_output=y_output.drop(['loan_status'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output['loan_status']=y_output['predict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output=y_output.drop(['predict'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output.to_csv('y_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_output.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test.loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>Y</td>\n",
       "      <td>1287501</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>Y</td>\n",
       "      <td>1043541</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>N</td>\n",
       "      <td>848320</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>N</td>\n",
       "      <td>951132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>N</td>\n",
       "      <td>268666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141446</th>\n",
       "      <td>141446</td>\n",
       "      <td>2132750</td>\n",
       "      <td>N</td>\n",
       "      <td>2049735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141447</th>\n",
       "      <td>141447</td>\n",
       "      <td>2132762</td>\n",
       "      <td>N</td>\n",
       "      <td>872753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141448</th>\n",
       "      <td>141448</td>\n",
       "      <td>2132790</td>\n",
       "      <td>N</td>\n",
       "      <td>178207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141449</th>\n",
       "      <td>141449</td>\n",
       "      <td>2132792</td>\n",
       "      <td>N</td>\n",
       "      <td>935</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141450</th>\n",
       "      <td>141450</td>\n",
       "      <td>2132799</td>\n",
       "      <td>N</td>\n",
       "      <td>946298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141451 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1 loan_status  Unnamed: 0.1.1  ans\n",
       "0                0             6           Y         1287501    1\n",
       "1                1            22           Y         1043541    1\n",
       "2                2            27           N          848320    0\n",
       "3                3            28           N          951132    0\n",
       "4                4            30           N          268666    0\n",
       "...            ...           ...         ...             ...  ...\n",
       "141446      141446       2132750           N         2049735    0\n",
       "141447      141447       2132762           N          872753    0\n",
       "141448      141448       2132790           N          178207    0\n",
       "141449      141449       2132792           N             935    0\n",
       "141450      141450       2132799           N          946298    0\n",
       "\n",
       "[141451 rows x 5 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train.loan_status.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1272824\n",
       "1         951395\n",
       "0        2322300\n",
       "1        1056876\n",
       "2         354427\n",
       "          ...   \n",
       "1301       21949\n",
       "14689     240440\n",
       "11227     183710\n",
       "8500      140218\n",
       "23857     390818\n",
       "Name: Unnamed: 0.1.1.1, Length: 533200, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_output['Unnamed: 0.1.1.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_output.loan_status.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
